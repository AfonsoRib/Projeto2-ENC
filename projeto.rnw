\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % habilita el uso de comentarios en varias lineas (\ifx \fi) 
\usepackage{lipsum} %Este paquete genera texto del tipo  Lorem Ipsum. 
\usepackage{fullpage} % cambia el margen

\usepackage{multicol}
\usepackage{caption}
\usepackage{mwe}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{arydshln}
\usepackage{cancel}
\usepackage{bm}
\usepackage{array}
    
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{alizarin}{rgb}{0.82, 0.1, 0.26}
\definecolor{shadecolor}{rgb}{1,.8,.3}
\definecolor{almond}{rgb}{0.94, 0.87, 0.8}

\newcommand*{\bigchi}{\mbox{\Large$\chi$}}

\makeatletter
\def\thickhline{%
  \noalign{\ifnum0=`}\fi\hrule \@height \thickarrayrulewidth \futurelet
   \reserved@a\@xthickhline}
\def\@xthickhline{\ifx\reserved@a\thickhline
               \vskip\doublerulesep
               \vskip-\thickarrayrulewidth
             \fi
      \ifnum0=`{\fi}}
\makeatother

\newlength{\thickarrayrulewidth}
\setlength{\thickarrayrulewidth}{4\arrayrulewidth}


\newtheorem{theorem}{Proposition}


\begin{document}

\hspace*{-\parindent}%
\begin{minipage}[t]{0.7\linewidth}
\vspace{-0.8cm}
\raggedright
	%\noindent
	\large\textbf{Project 2} \\
	\textbf{Computational Numerical Statistics} \\
	\normalsize DM, FCT-UNL \\
	\textbf{2022-2023}\\
	{\bf Group 6 : Afonso Ribeiro 59895, Francisco Freitas 60313,  Miguel Santos 64474 and Pedro Anjos 66519}
\end{minipage}
\begin{minipage}{0.3\linewidth}
\raggedleft
	\includegraphics[scale=0.12]{logo_nova-st_rgb_vert_positivo.png}
\end{minipage}
\\

\par\noindent\rule{\textwidth}{0.4pt}


\section{Introduction}

\subsection{Jackknife method}
The jackknife method is a resampling method with the goal of estimating the bias and standard error of estimators by systematically estimating $\theta$ using all the samples that can be obtained from the original observed sample by subtracting one sample value each time.\\ \\
Let $X_1, . . . , X_n$ be a random sample from some population $X \sim F(\theta)$, with both $F$ and $\theta$ unknown. Let $T = g(X_1, . . . , X_n)$ be an estimator of $\theta$. \\
Let $x_1, . . . , x_n$ be a realization of the random sample above and $t = g(x_1, . . . , x_n)$ an estimate of $\theta$.\\
Let\\ 
\centerline{
$
(x_2, x_3, \ldots, x_n), \:\: (x_1, x_3,\ldots, x_n)\:\:, \:\: \ldots \:\:, \:\:(x_1, \ldots, x_{n-1})
$} \\
be the $n$ jackknife samples (with size = ($n-1$)) and \\
\centerline{
$
t^{\ast}_1 = g(x_2, x_3, \ldots , x_n),\:\: \ldots \:\:, \:\: t^{\ast}_n = g(x_1, \ldots, x_{n-1})
$
}
be the $n$ jackknife estimates of $\theta$. \\
Let $t_{jack} = \frac{1}{n}\sum_{i=1}^n t^{\ast}_i = \overline{t}^{\ast}$ (jackknife estimate of $\theta$)

\section{Exercises}
\subsection{Problem 1}
When $T = \overline{X}$, show that: 
\subsubsection{$T = T_{jack}$ }
If $T = \overline{X}$ then $t = \overline{X}$ and \\
\begin{align*}
t^\ast_1 & = \frac{1}{n-1} (x_2 + \ldots + x_n) = \frac{1}{n-1} \sum^n_{i=1, i \neq 1}(x_i)\\
t^\ast_2 & = \frac{1}{n-1} (x_1 + x_3 + \ldots + x_n) = \frac{1}{n-1} \sum^n_{i=1, i \neq 2}(x_i)\\
\ldots \\
t^\ast_n & = \frac{1}{n-1} (x_1 + \ldots + x_{n-1}) = \frac{1}{n-1} \sum^n_{i=1, i \neq n}(x_i)
\end{align*}
\\
\textbf{Since} $T_{jack} =\frac{1}{n}\sum^n_{i=1}(t^\ast_i) = \overline{t^\ast}$, therefore \\
\begin{align*}
T_{jack} &= \frac{1}{n}\sum^n_{i=1}(t^\ast_i) = \frac{1}{n} \sum^n_{i=1}(\frac{1}{n-1} \sum^n_{j=1, j \neq i}(x_j))\\
& = \frac{1}{n} \frac{1}{n-1} \sum^n_{i=1}(\sum^n_{j=1, j \neq i}(x_j))
\end{align*}
\textbf{Considering that} \\
$\sum^n_{i=1}(c) = nc$ and $\sum^n_{i=1}(\sum^n_{j=1, j \neq i}(x_j)) = ((x_2+...+x_n)+ (x_1+x_3+ ...+x_n) + \ldots + (x_1+...+x_{n-1}))$ \\
\textbf{thus} \\
$\sum^n_{i=1}(\sum^n_{j=1, j \neq i}(x_j)) = (n-1)(x_1 + \ldots + x_n)$

\begin{align*}
T_{jack} &= \frac{1}{n} \frac{1}{n-1} \sum^n_{i=1}(\sum^n_{j=1, j \neq i}(x_j)) \\
& = \frac{1}{n}\frac{1}{n-1}\sum^n_{i=1}(n-1)(x_1 + \ldots + x_n) \\
\end{align*}
\begin{align*}
& = \frac{1}{n}(\frac{1}{\bcancel{(n-1)}}\bcancel{(n-1)}\sum^n_{i=1}(x_1 + \ldots + x_n) \\
& = \frac{1}{n} (x_1+...+x_n) \\
& = \overline{x} 
\end{align*}
Since $x_1, \ldots, x_n$ is a realization of $X_1, ..., X_n$ we can conclude that ${\bf T = T_{jack}}$

\subsubsection{$V(T_{jack}) = \frac{n-1}{n} \sum_{i=1}^{n} (T_i^*-T_{jack})^2$ simplifies to $\frac{S^2}{n}=V(\overline{X})$}
\begin{align*}
    V(T_{jack}) & = \frac{n-1}{n} \sum_{i=1}^{n} (T_i^*-T_{jack})^2 \\
    & = \frac{n-1}{n} ((\frac{1}{n-1}(x_2 + \ldots + x_n) - \overline{X})^2 + \ldots + (\frac{1}{n-1}(x_1 + \ldots + x_{n-1}) - \overline{X})^2) \\
    & = \frac{n-1}{n} ((\frac{1}{n-1}((x_2 + \ldots + x_n) - (n-1) \overline{X}))^2 + \ldots + (\frac{1}{n-1}((x_1 + \ldots + x_{n-1}) - (n-1)\overline{X}))^2) \\
    & = \frac{\bcancel{(n-1)}}{n} \frac{1}{(n-1)^{\bcancel{2}}}(((x_2 + \ldots + x_n) - (n-1) \overline{X})^2 + \ldots + ((x_1 + \ldots + x_{n-1}) - (n-1)\overline{X})^2) \\
    & = \frac{1}{n(n-1)}(((x_2 + \ldots + x_n) - (n\overline{X} - \overline{X}))^2 + \ldots + ((x_1 + \ldots + x_{n-1}) - (n\overline{X} - \overline{X}))^2) \\
    & = \frac{1}{n(n-1)}(((x_2 + \ldots + x_n) - n\overline{X} + \overline{X})^2 + \ldots + ((x_1 + \ldots + x_{n-1}) - n\overline{X} + \overline{X})^2)
\end{align*}
\pagebreak

\begin{align*}
& = \frac{1}{n(n-1)}(((x_2 + \ldots + x_n) - \bcancel{n}(\frac{1}{\bcancel{n}}\sum^n_{i=1}(x_i)) + \overline{X})^2 + \ldots + ((x_1 + \ldots + x_{n-1}) - \bcancel{n}(\frac{1}{\bcancel{n}}\sum^n_{i=1}(x_i)) + \overline{X})^2) \\
& = \frac{1}{n(n-1)}(((x_2 + \ldots + x_n) - (x_1 + \ldots + x_n) + \overline{X})^2 + \ldots + ((x_1 + \ldots + x_{n-1}) - (x_1 + \ldots + x_n) + \overline{X})^2) \\
& = \frac{1}{n(n-1)}((-x_1 + \overline{X})^2 + \ldots + (- x_n + \overline{X})^2) = \frac{1}{n(n-1)}((-(x_1 - \overline{X}))^2 + \ldots + (-(x_n - \overline{X}))^2) \\
& = \frac{1}{n(n-1)}((-1)^2(x_1 - \overline{X})^2 + \ldots + (-1)^2(x_n - \overline{X})^2) = \frac{1}{n(n-1)}((x_1 - \overline{X})^2 + \ldots + (x_n - \overline{X})^2) \\
& = \frac{1}{n(n-1)}\sum^n_{i=1}(x_i - \overline{X})^2 \\
\end{align*}
\textbf{Considering that} 
$S^2 = (\sqrt{\frac{\sum^n_{i=1}(x_i - \overline{X})^2}{n-1}})^2 = \frac{\sum^n_{i=1}(x_i - \overline{X})^2}{n-1}$\textbf{, thus}
\begin{align*}
    V(T_{jack}) & = \frac{1}{n(n-1)}\sum^n_{i=1}(x_i - \overline{X})^2 \\
    & = \frac{S^2}{n} = V(\overline{X})
\end{align*}


\subsection{Consider the observed sample referring to the survival times of some electrical component pertaining to a car assembly factory.}

\begin{center}
\begin{tabular}{c c c c c c c c c c}
  $1552$ & $627$ & $884$ & $2183$ & $1354$ & $1354$ & $1014$ & $2420$ & $71$ & $3725$ \\
  $2195$ & $2586$ & $1577$ & $1766$ & $1325$ & $1299$ & $159$ & $1825$ & $965$ & $695$ 
\end{tabular}
\end{center}

\subsubsection{(c) Compute the 90\% BCa bootstrap CI for $\mu$.}

\textbf{Bias-Corrected-accelereated bootstrap confidence interval}

The Bias-Corrected-accelereated bootstrap confidence interval (abbreviated often to BCa) is an improved version of the percentile method to obtain the confidence interval. Altough oftentimes it gives better results it can still be can give erratic results on small sample sizes. It corrects for bias and skewness in the distribution of bootstrap estimates.

Starting off we will need the bias correction parameter $z_0$ which is directly obtained from the proportion of bootstrap samples less than the original estimate $\hat{\theta}$:

\[\hat{z}_0 = \Phi^{-1} \left( \frac{\#\{\hat{\theta}^* < \hat{\theta}\}}{B} \right)\]

Where $\phi^{-1}$ is the inverse function of the standard normal cumulative function distribution function and $\hat{\theta}^*$ is the boostrap samples.

After that we will the need the acceleration $\hat{a}$. There are various ways to calculate the acceleration but we will use the jackknife method for simplicity. Let $\hat{\theta} = s(x)$ be the statistic we want to calculate. let $x_{(-i)}$ be the original sample with the value with index i removed, let $\hat{\theta}_{(-i)} = s(x_{(-i)})$ and $\hat{\theta}_{(.)} = \sum_{i=1}^{n}(\hat{\theta}_{(-i)})/n$. we can now calculate the accelaration with the following expression;

\[\hat{a} = \frac{\sum_{i=1}^{n}(\hat{\theta}_{(.)} - \hat{\theta}_{(-i)})^3}{6\{\sum_{i=1}^{n}(\hat{\theta}_{(.)} - \hat{\theta}_{(-i)})^2\}^{3/2}}\]

finally, having both $\hat{a}$ and $\hat{z}_0$ we can now calculate the adjusted intervals endpoint $a_1$ and $a_2$ with the following expressions:

\[ a_1 = \Phi \left( \hat{z}_0 + \frac{\hat{z}_0 + z^{(\alpha)}}{1-\hat{a}(\hat{z}_0 + z^{(\alpha)})}     \right)\]

\[ a_2 = \Phi \left( \hat{z}_0 + \frac{\hat{z}_0 + z^{(1-\alpha)}}{1-\hat{a}(\hat{z}_0 + z^{(1-\alpha)})}\right)\]

where $\Phi$ the standard normal cumulative distribution function and $z^{(i)}$ the 100$\alpha$th percentile point of a standard normal  distribution.

\subsubsection{(e) Show that $\mathcal{P} = \frac{X}{n}$ is an unbiased and consistent estimator of p. Estimate p and SE(P).}
\begin{center}
  $E(\mathcal{P}) = E\left(\frac{X}{n}\right) = \frac{E(X)}{n}$
\end{center}

X has an Binomial distribution therefore its expected value is $np$. Proof:


\begin{align*}
  \sum_{k=1}^{n} k \binom{n}{k} p^K (1-p)^{n-k} = \\
  \sum_{k=1}^{n} k \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} = \\
  \sum_{k=1}^{n} \frac{n!}{(k-1)!(n-k)!} p^k (1-p)^{n-k} = \\
  np \sum_{k=1}^{n} \frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} (1-p)^{n-k} = \\
  np \sum_{k=1}^{n} \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k}
\end{align*}

through the binomial theorem we get:

\[np(p + (1-p))^{n-1} = np * 1 = np\]

Knowing this:
\begin{center}
$\frac{E(X)}{n} = \frac{np}{n} = p $ 
\end{center}

since $E(\mathcal{P}) = p$ whe can afirm that $\mathcal{P}$ is an unbiased estimator.

\[  \lim_{n\to\infty} V(\mathcal{P}) = \lim_{n\to\infty} V\left(\frac{X}{n}\right) = \lim_{n\to\infty} \frac{V(X)}{n^2}  = 0\]

$\mathcal{P}$ is a consistent estimator since it's variance aproaches 0 as the sample size increases.


\subsubsection{(f) Describe and discuss in detail the non-parametric bootstrap and jackknife techniques. Use both approaches (B = 10000 samples in the case of the bootstrap) to estimate the variance, standard error and bias of $\mathcal{P}$. Compare the results. Check whether there is need to correct the original estimate of p for bias and if such report the corrected estimate of p.}

\textbf{non-parametric bootstraping} \\
Bootstaping is any resampling method that that uses random sampling with replacement for a given sample. It is used to assess the accuracy of statistical estimates and tests and estimate statistics of the population.

The non-parametric does not make any assumptions about the distribution of the population. This makes it on average more robust to distributional assumptions unlike the parametric bootstrap. With this robustness comes a necessity for a larger sample size than what would be needed for a parametric bootstrap. If the sample size is too small the estimator it might not approximate the real value very well.

Here is algorithm for the non-parametric bootstrap:

\begin{center}
  \begin{varwidth}{\textwidth}
  \begin{itemize}
  \item[step 1 -] choose the the amount of bootstrap samples to compute
  \item[step 2 -] for each bootstrap sample
    \begin{itemize}
    \item[step 1 -] sample the population with replacement
    \item[step 2 -] calculate the statistic you want to obtain
    \end{itemize}
  \item[step 3 -] calculate the mean of the the obtain bootstrap samples
  \end{itemize}
  \end{varwidth}
\end{center}

Bootstraping is really simple and easy to understand. It can be applied to complex estimators to derive estimates for the standard error and confidence intervals. Bootstrapping also avoids the cost of gathering new sample data.

Bootstaping is heavly dependent on the estimator. Using it without care can lead to inconsistent results. When using bootstrap, assumptions about the about the sample are made and it does not garantee that values are representitive of the general population. Bootstrapping can be time consuming and it's not easly automatable using statistical computer packages.\\

\textbf{jackknife} \\

The jackknife is another resampling technique.It predates the bootstrap. It was created with the goal of estimating the bias and standard error estimators. The jackknife allows us to obtain samples similar to the original sample by leaving one sample value out at a time, unlike the bootstrap method.

Here is the jackknife algorihtm:

\begin{center}
  \begin{varwidth}{\textwidth}
    \begin{itemize}

    \item Let $X_1 ,..., X_n$ be a random sample from some population $X \sim F (\theta$) with both $F$ and $\theta$ unknown. Let $T = g(X_1 ,..., X_n)$ be an estimator of $\theta$.

    \item Let $x_1 ,..., x_n$ be a realization of the random sample above and $t = g(x_1 ,..., x_n )$ an estimate of $\theta$.

    \item Let
      \begin{center}
        \begin{tabular}{c c c c}
          $x_2 , x_3 ,..., x_n ,$ & $x_1 , x_3 ,..., x_n,$ & ... , & $x_1 ,..., x_{n−1}$
        \end{tabular}

      \end{center}
      be the n jackknife samples (of size $(n - 1)$) and
      \begin{center}
        \begin{tabular}{c c c }
          $t{_1^*} = g(x_2 , x_3 ,..., x_n ),$ &
          ... , & 
          $t{_n^*} = g(x_1 ,..., x_{n−1} )$ 
        \end{tabular}
      \end{center}
      be the $n$ jackknife estimates of $\theta$

    \item let $t_{jack} = \frac{1}{n} \sum_{i=1}^{n} t{_i^*} = \bar{t^*}$ (jacknife estimation of $\theta$)


  \end{itemize}
  \end{varwidth}
\end{center}

It is normally less computational itensive than the bootstrap method. like the bootstrap, it does not make any assumptions about the distribution of the sample. It is easier than the bootstrap to apply to complex sampling schemes than the bootstrap.

Jackknife only works well for functoins with small changes in the statistic therefore it works well with, for example, linear statistics. May that not be the  case, it might fail to estimate the variance successfully. Compared to the boostrap, jackknife is only a crude approximation and therefore, if a more accurate value is desired, bootstrap confidence intervals are a better choice.

\subsection{(g) The jackknife-after-bootstrap technique provides a way of measuring the uncertainty associated with the bootstrap estimate $\widehat{SE}(T)$ ($T$ some estimator of interest). Use this technique in order to estimate the standard error of the bootstrap estimate of $SE(P)$ obtained in (d).}

\textbf{jackknife-after-bootstrap} \\

Jackknife-after-boostrap is a method used to find the error estimate of a bootstrap estimate. It can be applied to any bootstrap statistic, not only standard error. After bootstrapping, we apply the jackknife to each of the newly generated samples. 

Here is the algorithm for the calculating the variance statistic, after drawing the boostrap samples:

\begin{itemize}
\item For $i = 1,2, ..., n$, leave out the data in index i. call the result $\widehat{SE}_{B(i)}$
\item define  \[\widehat{var}_{jack}(\widehat{se}_B) = (n-1) \frac{\sum_{1}^{n}(\widehat{se}_{B(i)} - \widehat{se}_{B(.)})^2}{n},\] where $\widehat{se}_{B(.)} = \sum_{i=1}^{n} \widehat{se}_{B(i)}/n$ (the mean of the boostrap standard error).
\end{itemize}

\bibliographystyle{unsrt}
\bibliography{bibs}
\end{document}
